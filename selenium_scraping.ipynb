{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import imporant library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import tls_client\n",
    "from seleniumbase import SB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get raw data we want to find based on\n",
    "\\\n",
    "***run \"Load_json(any)\" cell in <u>clean.ipynb</u> to get \"raw_data.csv\" first***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunda\\AppData\\Local\\Temp\\ipykernel_11772\\3966301290.py:1: DtypeWarning: Columns (89,156,157,161,180,183,184,186,187,189,190,191,192,193,210,213,241,244,246,247,248,263,267,268,269,270,271,273,279,280,285,290,291,292,293,295,298,299,300,301,303,304,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  init_data = pd.read_csv(\"raw_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "init_data = pd.read_csv(\"raw_data.csv\")\n",
    "viz_data = pd.read_parquet(\"viz_data.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites used\n",
    "\n",
    "- https://www.researchgate.net for [references, citations]\n",
    "  1. go to \"/search\"\n",
    "  2. input using \"{journal title} OR {author}\"\n",
    "  3. look for matching, click it\n",
    "  4. find desired content\n",
    "\n",
    "- https://service.elsevier.com for [subject code]\n",
    "  1. get html content from \"/app/answers/detail/a_id/15181/supporthub/scopus/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing journal stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making search_key for website search bar (researchgate.net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# missing references or citations\n",
    "ref_count = \"abstracts-retrieval-response.item.bibrecord.tail.bibliography.@refcount\"\n",
    "cit_count = \"abstracts-retrieval-response.coredata.citedby-count\"\n",
    "title = \"abstracts-retrieval-response.coredata.dc:title\"\n",
    "author = \"abstracts-retrieval-response.coredata.dc:creator.author\"\n",
    "doi = \"abstracts-retrieval-response.coredata.prism:doi\"\n",
    "\n",
    "missing = init_data[init_data[ref_count].isna() | init_data[cit_count].isna()]\n",
    "sample = missing[[title, author, doi]]\n",
    "\n",
    "\n",
    "# making search key\n",
    "def extract_author_name(dict_str):\n",
    "    res = eval(dict_str)\n",
    "    name = res[0].get(\"preferred-name\")\n",
    "    return name.get(\"ce:surname\", \"\") + \" \" + name.get(\"ce:given-name\", \"\")\n",
    "\n",
    "sample[author] = sample[author].apply(lambda x: extract_author_name(x))\n",
    "sample[\"search_key\"] = sample[author] + \" OR \" + '\"' + sample[title] + '\"'\n",
    "\n",
    "\n",
    "# convert to list for scraping\n",
    "sample[doi] = sample[doi].fillna(\"\")\n",
    "search_keys = sample[\"search_key\"].to_list()\n",
    "titles = sample[title].to_list()\n",
    "dois = sample[doi].to_list()\n",
    "rows = sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.2420.81\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14.4; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "]\n",
    "\n",
    "def get_journal_links(dois , titles , search_keys):\n",
    "    #find most likely journal link from provided parameters\n",
    "    urls = []\n",
    "    for i in range(len(titles)):\n",
    "        time.sleep(random.uniform(1,3))\n",
    "        \n",
    "        # html from request\n",
    "        header = {\"User-Agent\": agent_list[i % 10]}\n",
    "        session = tls_client.Session(\n",
    "            client_identifier=\"chrome112\", random_tls_extension_order=True\n",
    "        )\n",
    "        res = session.get(\n",
    "            f\"https://www.researchgate.net/search/publication?q={search_keys[i]}\",\n",
    "            headers=header,\n",
    "        )\n",
    "        session.close()\n",
    "\n",
    "        search_result_html = BeautifulSoup(res.content, \"lxml\")\n",
    "        searched_title = search_result_html.select(\n",
    "            \"div.nova-legacy-v-publication-item__title a\"\n",
    "        )\n",
    "\n",
    "        # no search result found\n",
    "        if len(searched_title) == 0:\n",
    "            urls.append(\"\")\n",
    "            continue\n",
    "\n",
    "        idx = -1\n",
    "        if dois[i] != \"\":\n",
    "        # match by DOI\n",
    "            meta_datas = search_result_html.select(\n",
    "                \"ul.nova-legacy-v-publication-item__meta-data li:nth-child(2) span\"\n",
    "            )\n",
    "\n",
    "            #all DOIs from search results\n",
    "            searched_doi = [(i.text.split(\" \")[1]) for i in meta_datas]\n",
    "            try:\n",
    "                #check if one match\n",
    "                idx = searched_doi.index(dois[i])\n",
    "            except:\n",
    "                #impossible to find, skip\n",
    "                urls.append(\"\")\n",
    "                continue\n",
    "        else:\n",
    "        # match by title\n",
    "            #all titles from search results\n",
    "            searched_title_text = [i.text for i in searched_title]\n",
    "            #look for similarity\n",
    "            for j in range(len(searched_title_text)):\n",
    "                if titles[i] in searched_title_text[j]:\n",
    "                    idx = j\n",
    "                    break\n",
    "\n",
    "        if idx==-1:\n",
    "            urls.append(\"\")\n",
    "            continue\n",
    "        \n",
    "        # get link from matched result\n",
    "        href = searched_title[idx].get_attribute_list(\"href\", \"\")\n",
    "        \n",
    "        print(f\"{i}: {href[0]}\")\n",
    "        urls.append(href[0])\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def get_content(urls):\n",
    "    ref_count = []\n",
    "    cite_count = []\n",
    "\n",
    "    for i in range(len(urls)):\n",
    "        \n",
    "        print(f\"{(i/rows)*100.0:.2f}%...\")\n",
    "        \n",
    "        # skip if don't have link\n",
    "        if urls[i] == \"\":\n",
    "            ref_count.append(np.nan)\n",
    "            cite_count.append(np.nan)\n",
    "            print(\":Url not provided:\")\n",
    "            continue\n",
    "\n",
    "        with SB(uc=True) as sb:\n",
    "            sb.uc_open_with_reconnect(\n",
    "                f\"https://www.researchgate.net/{urls[i]}\",\n",
    "                3,\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            # dealing with CAPTCHA\n",
    "            try:\n",
    "                if sb.is_element_visible('input[value*=\"Verify\"]'):\n",
    "                    sb.uc_click('input[value*=\"Verify\"]')\n",
    "                else:\n",
    "                    sb.uc_gui_click_captcha()\n",
    "            except Exception:\n",
    "                print(\"No captcha appeared\")\n",
    "\n",
    "            try:\n",
    "                result = sb.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"div.publication-details__section h2.nova-legacy-e-text.nova-legacy-e-text--size-m.nova-legacy-e-text--family-display.nova-legacy-e-text--spacing-none.nova-legacy-e-text--color-inherit\",\n",
    "                )\n",
    "                cite_text = result[0].text\n",
    "                ref_text = result[1].text\n",
    "                refc = int(ref_text[ref_text.find(\"(\") + 1 : ref_text.rfind(\")\")])\n",
    "                citec = int(cite_text[cite_text.find(\"(\") + 1 : cite_text.rfind(\")\")])\n",
    "\n",
    "                ref_count.append(refc)\n",
    "                cite_count.append(citec)\n",
    "            except:\n",
    "                ref_count.append(np.nan)\n",
    "                cite_count.append(np.nan)\n",
    "                print(\":Failed to gathering:\")\n",
    "    return ref_count,cite_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_journal_links(dois=dois , titles=titles , search_keys=search_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = pd.DataFrame(links,columns=[\"href_to_journal\"])\n",
    "link_df.to_csv(\"missing_count_journal_href.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_c , cite_c = get_content(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to dataframe with key and export\n",
    "count_df = pd.DataFrame({\"references_count\":ref_c , \"citations_count\":cite_c , \"title\":missing[title] , \"author\":missing[author] , \"doi\":missing[doi]})\n",
    "count_df.to_csv(\"ref_cite_count.csv\")\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get journal subject code from Elsevier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus/\"\n",
    "response = rq.get(url)\n",
    "html = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "header = html.select(\"table.order-table thead tr th\")\n",
    "body = html.select(\"table.order-table tbody tr\")\n",
    "\n",
    "label = [i.select_one(\"h3\").text for i in header]\n",
    "data = [[j.text for j in i.select(\"td p\")] for i in body]\n",
    "\n",
    "code_df = pd.DataFrame(data, columns=label)\n",
    "code_df.to_csv(\"ASJC_cat.csv\")\n",
    "code_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Coordinates from Country name using 'Geocode HERE' API\n",
    "Country of affiliations with that journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://geocode.search.hereapi.com/v1/geocode\"\n",
    "HERE_API_KEY = \"aByISMRAsDgzQPiDivruVUOqEpOMkvIIL1y84zFrHYY\"\n",
    "\n",
    "#get country list\n",
    "country_exploded = viz_data.explode(\"affiliations_country\")\n",
    "country_list = country_exploded[\"affiliations_country\"].unique()\n",
    "country_list = country_list[np.where(country_list != None)]\n",
    "lng = []\n",
    "lat = []\n",
    "for country in country_list:\n",
    "    if country is None:\n",
    "        #skip nan value\n",
    "        continue\n",
    "    session = tls_client.Session()\n",
    "    response = session.get(base_url , params = {\"q\":country,\"apiKey\":HERE_API_KEY} )\n",
    "    print(response.content)\n",
    "    data = json.loads(response.content)\n",
    "    coord = data[\"items\"][0][\"position\"]\n",
    "    lng.append(coord[\"lng\"])\n",
    "    lat.append(coord[\"lat\"])\n",
    "    session.close()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "coord_df = pd.DataFrame({\"country_name\":country_list , \"latitude\": lat, \"longitude\":lng })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_df.to_csv('coordinate_country.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
